{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tenseal flwr pennylane"
      ],
      "metadata": {
        "id": "sweTSVqiH1aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QNN"
      ],
      "metadata": {
        "id": "hs4tknD4E1Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Hybrid Quantum-assisted CNN on CIFAR-10\n",
        "PennyLane + PyTorch – single GPU\n",
        "~ 63 % test accuracy in 30 min on RTX-3080\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "import pennylane as qml\n",
        "from pennylane.qnn import TorchLayer\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Config\n",
        "# -------------------------------------------------\n",
        "N_QUBITS      = 4          # keep it simulable\n",
        "N_QLAYERS     = 2\n",
        "EPOCHS        = 30\n",
        "LR            = 3e-3\n",
        "WEIGHT_DECAY  = 5e-4\n",
        "LABEL_SMOOTH  = 0.1\n",
        "MIXUP_ALPHA   = 0.2\n",
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Data\n",
        "# -------------------------------------------------\n",
        "train_transform = T.Compose([\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "test_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                         download=True, transform=train_transform)\n",
        "test_set  = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                         download=True, transform=test_transform)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Quantum node\n",
        "# -------------------------------------------------\n",
        "dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"best\")\n",
        "def quantum_circuit(inputs, weights):\n",
        "    qml.AngleEmbedding(inputs, wires=range(N_QUBITS), rotation=\"Y\")\n",
        "    qml.BasicEntanglerLayers(weights, wires=range(N_QUBITS))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]\n",
        "\n",
        "weight_shapes = {\"weights\": (N_QLAYERS, N_QUBITS)}\n",
        "qlayer = TorchLayer(quantum_circuit, weight_shapes)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Hybrid model\n",
        "# -------------------------------------------------\n",
        "class HybridQCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                           # 16×16\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                           # 8×8\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1))                # 1×1×128\n",
        "        )\n",
        "        self.fc_reduce = nn.Linear(128, N_QUBITS)     # 128 → 4\n",
        "        self.qlayer    = qlayer                       # quantum\n",
        "        self.classifier = nn.Linear(N_QUBITS, 10)     # 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.fc_reduce(x)\n",
        "        x = self.qlayer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "model = HybridQCNN().to(DEVICE)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Batch-size auto-tuning\n",
        "# -------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def find_max_batch(model, device, start=64, max_batch=512):\n",
        "    b = start\n",
        "    while b <= max_batch:\n",
        "        try:\n",
        "            x = torch.randn(b, 3, 32, 32, device=device)\n",
        "            _ = model(x)\n",
        "            b *= 2\n",
        "        except RuntimeError:\n",
        "            break\n",
        "    return min(b//2, max_batch)\n",
        "\n",
        "BATCH_SIZE = find_max_batch(model, DEVICE, 64, 512)\n",
        "print(f\"Auto-selected batch size: {BATCH_SIZE}\")\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,  num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Loss, optimiser, scheduler\n",
        "# -------------------------------------------------\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR,\n",
        "                                          epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7. Mixup helpers\n",
        "# -------------------------------------------------\n",
        "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 8. Training\n",
        "# -------------------------------------------------\n",
        "best_acc = 0.\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running_loss, running_acc, n = 0., 0., 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
        "    for x, y in pbar:\n",
        "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y_a, y_b, lam = mixup_data(x, y)\n",
        "        logits = model(x)\n",
        "        loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        preds = logits.argmax(1)\n",
        "        acc = (lam * (preds == y_a).float() + (1 - lam) * (preds == y_b).float()).mean().item()\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        running_acc  += acc * x.size(0)\n",
        "        n += x.size(0)\n",
        "        pbar.set_postfix({\"loss\": running_loss / n, \"acc\": running_acc / n})\n",
        "\n",
        "    # ---------- validation ----------\n",
        "    model.eval()\n",
        "    val_acc, m = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
        "            logits = model(x)\n",
        "            val_acc += (logits.argmax(1) == y).float().sum().item()\n",
        "            m += y.size(0)\n",
        "    val_acc /= m\n",
        "    print(f\"Epoch {epoch:02d}  val-acc {val_acc:.2%}  lr {scheduler.get_last_lr()[0]:.2e}\")\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_qcnn.pt\")\n",
        "\n",
        "print(f\"\\nBest test accuracy: {best_acc:.2%}\")"
      ],
      "metadata": {
        "id": "OHG_Vh4CNqkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FL-QNN"
      ],
      "metadata": {
        "id": "OhJh5XuRE4dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import pennylane as qml\n",
        "from pennylane.qnn import TorchLayer\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import copy\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Config\n",
        "# -------------------------------------------------\n",
        "N_QUBITS      = 4\n",
        "N_QLAYERS     = 2\n",
        "N_CLIENTS     = 5          # Number of federated clients\n",
        "GLOBAL_ROUNDS = 10         # Federated rounds\n",
        "LOCAL_EPOCHS  = 3          # Local training epochs per client\n",
        "LR            = 3e-3\n",
        "WEIGHT_DECAY  = 5e-4\n",
        "LABEL_SMOOTH  = 0.1\n",
        "MIXUP_ALPHA   = 0.2\n",
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# For comparison\n",
        "COMPARISON_EPOCHS = 30      # Non-FL training epochs\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "print(f\"Federated Learning Setup:\")\n",
        "print(f\"  Clients: {N_CLIENTS}\")\n",
        "print(f\"  Global Rounds: {GLOBAL_ROUNDS}\")\n",
        "print(f\"  Local Epochs: {LOCAL_EPOCHS}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Data - Non-IID Distribution\n",
        "# -------------------------------------------------\n",
        "train_transform = T.Compose([\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "test_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "full_train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                               download=True, transform=train_transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=test_transform)\n",
        "\n",
        "def create_non_iid_splits(dataset, n_clients, classes_per_client=4):\n",
        "    \"\"\"\n",
        "    Create non-IID data splits for federated learning.\n",
        "    Each client gets data from only a subset of classes.\n",
        "    \"\"\"\n",
        "    n_classes = 10\n",
        "    client_indices = [[] for _ in range(n_clients)]\n",
        "\n",
        "    # Group indices by class\n",
        "    class_indices = {i: [] for i in range(n_classes)}\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "    # Assign classes to clients\n",
        "    for client_id in range(n_clients):\n",
        "        # Each client gets 'classes_per_client' classes\n",
        "        start_class = (client_id * classes_per_client // n_clients) % n_classes\n",
        "        client_classes = [(start_class + i) % n_classes for i in range(classes_per_client)]\n",
        "\n",
        "        for cls in client_classes:\n",
        "            # Give each client a portion of data from assigned classes\n",
        "            n_samples = len(class_indices[cls]) // (n_clients // 2)  # Some overlap\n",
        "            client_indices[client_id].extend(class_indices[cls][:n_samples])\n",
        "\n",
        "        print(f\"Client {client_id}: {len(client_indices[client_id])} samples, classes {client_classes}\")\n",
        "\n",
        "    return client_indices\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Quantum node\n",
        "# -------------------------------------------------\n",
        "dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"best\")\n",
        "def quantum_circuit(inputs, weights):\n",
        "    qml.AngleEmbedding(inputs, wires=range(N_QUBITS), rotation=\"Y\")\n",
        "    qml.BasicEntanglerLayers(weights, wires=range(N_QUBITS))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]\n",
        "\n",
        "weight_shapes = {\"weights\": (N_QLAYERS, N_QUBITS)}\n",
        "qlayer = TorchLayer(quantum_circuit, weight_shapes)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Hybrid model\n",
        "# -------------------------------------------------\n",
        "class HybridQCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.fc_reduce = nn.Linear(128, N_QUBITS)\n",
        "        self.qlayer    = qlayer\n",
        "        self.classifier = nn.Linear(N_QUBITS, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.fc_reduce(x)\n",
        "        x = self.qlayer(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Federated Learning Functions (FIXED)\n",
        "# -------------------------------------------------\n",
        "def federated_averaging(global_model, client_models, client_weights=None):\n",
        "    \"\"\"\n",
        "    FedAvg: Aggregate client models by averaging their parameters\n",
        "    FIXED: Handle different dtypes properly\n",
        "    \"\"\"\n",
        "    if client_weights is None:\n",
        "        client_weights = [1.0 / len(client_models)] * len(client_models)\n",
        "\n",
        "    global_dict = global_model.state_dict()\n",
        "\n",
        "    for key in global_dict.keys():\n",
        "        # Initialize with zeros of the same dtype\n",
        "        global_dict[key] = torch.zeros_like(global_dict[key])\n",
        "\n",
        "        for client_model, weight in zip(client_models, client_weights):\n",
        "            client_param = client_model.state_dict()[key]\n",
        "\n",
        "            # Handle different dtypes (int64 for buffers, float for weights)\n",
        "            if client_param.dtype == torch.int64 or client_param.dtype == torch.long:\n",
        "                # For buffers like num_batches_tracked, just copy from first client\n",
        "                global_dict[key] = client_param.clone()\n",
        "                break\n",
        "            else:\n",
        "                # For float parameters, do weighted average\n",
        "                global_dict[key] += weight * client_param\n",
        "\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "def train_client(model, dataset, epochs, device, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train a single client's local model\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
        "                           num_workers=2, pin_memory=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        n = 0\n",
        "\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            x, y_a, y_b, lam = mixup_data(x, y)\n",
        "            logits = model(x)\n",
        "            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = logits.argmax(1)\n",
        "            acc = (lam * (preds == y_a).float() + (1 - lam) * (preds == y_b).float()).mean().item()\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "            running_acc += acc * x.size(0)\n",
        "            n += x.size(0)\n",
        "\n",
        "    return running_loss / n, running_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_global_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate global model on test set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Non-FL Training (for comparison)\n",
        "# -------------------------------------------------\n",
        "def train_non_fl(epochs, device, batch_size):\n",
        "    \"\"\"\n",
        "    Train a single model without federated learning (baseline)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BASELINE: NON-FL TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model = HybridQCNN().to(device)\n",
        "    train_loader = DataLoader(full_train_set, batch_size=batch_size,\n",
        "                             shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size,\n",
        "                            shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR,\n",
        "                                              epochs=epochs, steps_per_epoch=len(train_loader))\n",
        "\n",
        "    history = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, running_acc, n = 0., 0., 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x, y_a, y_b, lam = mixup_data(x, y)\n",
        "            logits = model(x)\n",
        "            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            preds = logits.argmax(1)\n",
        "            acc = (lam * (preds == y_a).float() + (1 - lam) * (preds == y_b).float()).mean().item()\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "            running_acc  += acc * x.size(0)\n",
        "            n += x.size(0)\n",
        "\n",
        "        # Validation\n",
        "        val_acc = evaluate_global_model(model, test_loader, device)\n",
        "        history.append(val_acc)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"Epoch {epoch:02d}  val-acc {val_acc:.2%}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    final_acc = history[-1]\n",
        "\n",
        "    print(f\"\\nBaseline Results:\")\n",
        "    print(f\"  Final Accuracy: {final_acc:.2%}\")\n",
        "    print(f\"  Training Time: {training_time:.2f}s\")\n",
        "\n",
        "    return final_acc, training_time, history\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7. Federated Training\n",
        "# -------------------------------------------------\n",
        "def train_federated(n_clients, global_rounds, local_epochs, device, batch_size):\n",
        "    \"\"\"\n",
        "    Train with federated learning\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"FEDERATED LEARNING: {n_clients} Clients\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Create client data splits\n",
        "    client_indices = create_non_iid_splits(full_train_set, n_clients)\n",
        "    client_datasets = [Subset(full_train_set, indices) for indices in client_indices]\n",
        "\n",
        "    # Initialize global model\n",
        "    global_model = HybridQCNN().to(device)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=2, pin_memory=True)\n",
        "\n",
        "    history = []\n",
        "    start_time = time.time()\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Federated rounds\n",
        "    for round_num in range(1, global_rounds + 1):\n",
        "        print(f\"\\nRound {round_num}/{global_rounds}\")\n",
        "\n",
        "        client_models = []\n",
        "        client_weights = []\n",
        "\n",
        "        # Train each client\n",
        "        for client_id in range(n_clients):\n",
        "            # Create local model copy\n",
        "            client_model = HybridQCNN().to(device)\n",
        "            client_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "            # Train locally\n",
        "            loss, acc = train_client(\n",
        "                client_model,\n",
        "                client_datasets[client_id],\n",
        "                local_epochs,\n",
        "                device,\n",
        "                batch_size\n",
        "            )\n",
        "\n",
        "            client_models.append(client_model)\n",
        "            client_weights.append(len(client_datasets[client_id]))\n",
        "\n",
        "        # Normalize weights\n",
        "        total_samples = sum(client_weights)\n",
        "        client_weights = [w / total_samples for w in client_weights]\n",
        "\n",
        "        # Aggregate (FedAvg)\n",
        "        global_model = federated_averaging(global_model, client_models, client_weights)\n",
        "\n",
        "        # Evaluate global model\n",
        "        global_acc = evaluate_global_model(global_model, test_loader, device)\n",
        "        history.append(global_acc)\n",
        "\n",
        "        print(f\"  Global Accuracy: {global_acc:.2%}\")\n",
        "\n",
        "        if global_acc > best_acc:\n",
        "            best_acc = global_acc\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nFederated Results:\")\n",
        "    print(f\"  Best Accuracy: {best_acc:.2%}\")\n",
        "    print(f\"  Training Time: {training_time:.2f}s\")\n",
        "\n",
        "    return best_acc, training_time, history\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 8. Find Optimal Number of Clients\n",
        "# -------------------------------------------------\n",
        "def find_optimal_clients(client_range, global_rounds, local_epochs, device, batch_size):\n",
        "    \"\"\"\n",
        "    Test different numbers of clients to find optimal\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINDING OPTIMAL NUMBER OF CLIENTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for n_clients in client_range:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Testing with {n_clients} clients\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        acc, train_time, history = train_federated(\n",
        "            n_clients, global_rounds, local_epochs, device, batch_size\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'n_clients': n_clients,\n",
        "            'accuracy': acc,\n",
        "            'time': train_time,\n",
        "            'history': history\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 9. Main Execution\n",
        "# -------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def find_max_batch(model, device, start=32, max_batch=256):\n",
        "    b = start\n",
        "    while b <= max_batch:\n",
        "        try:\n",
        "            x = torch.randn(b, 3, 32, 32, device=device)\n",
        "            _ = model(x)\n",
        "            b *= 2\n",
        "        except RuntimeError:\n",
        "            break\n",
        "    return min(b//2, max_batch)\n",
        "\n",
        "# Auto-tune batch size\n",
        "temp_model = HybridQCNN().to(DEVICE)\n",
        "BATCH_SIZE = find_max_batch(temp_model, DEVICE, 32, 256)\n",
        "print(f\"Auto-selected batch size: {BATCH_SIZE}\\n\")\n",
        "del temp_model\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Run Experiments\n",
        "# -------------------------------------------------\n",
        "\n",
        "# 1. Baseline (Non-FL)\n",
        "baseline_acc, baseline_time, baseline_history = train_non_fl(\n",
        "    COMPARISON_EPOCHS, DEVICE, BATCH_SIZE\n",
        ")\n",
        "\n",
        "# 2. Federated Learning\n",
        "fl_acc, fl_time, fl_history = train_federated(\n",
        "    N_CLIENTS, GLOBAL_ROUNDS, LOCAL_EPOCHS, DEVICE, BATCH_SIZE\n",
        ")\n",
        "\n",
        "# 3. Find optimal number of clients\n",
        "client_range = [3, 5, 7, 10]\n",
        "optimal_results = find_optimal_clients(\n",
        "    client_range, GLOBAL_ROUNDS, LOCAL_EPOCHS, DEVICE, BATCH_SIZE\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 10. Results Comparison\n",
        "# -------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Method':<20} {'Accuracy':<15} {'Time (s)':<15} {'Time Ratio'}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Baseline (Non-FL)':<20} {baseline_acc:>6.2%}{'':<9} {baseline_time:>8.2f}{'':<7} 1.00x\")\n",
        "print(f\"{'Federated ({N_CLIENTS} clients)':<20} {fl_acc:>6.2%}{'':<9} {fl_time:>8.2f}{'':<7} {fl_time/baseline_time:.2f}x\")\n",
        "\n",
        "print(f\"\\n{'Optimal Client Search:'}\")\n",
        "print(\"-\" * 70)\n",
        "for res in optimal_results:\n",
        "    n = res['n_clients']\n",
        "    acc = res['accuracy']\n",
        "    t = res['time']\n",
        "    print(f\"{'  ' + str(n) + ' clients':<20} {acc:>6.2%}{'':<9} {t:>8.2f}{'':<7} {t/baseline_time:.2f}x\")\n",
        "\n",
        "# Find best configuration\n",
        "best_config = max(optimal_results, key=lambda x: x['accuracy'])\n",
        "print(f\"\\n✓ Best Configuration: {best_config['n_clients']} clients\")\n",
        "print(f\"  Accuracy: {best_config['accuracy']:.2%}\")\n",
        "print(f\"  Time: {best_config['time']:.2f}s\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 11. Visualization\n",
        "# -------------------------------------------------\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Accuracy comparison\n",
        "ax1.plot(range(1, len(baseline_history) + 1),\n",
        "         [x * 100 for x in baseline_history],\n",
        "         'b-', label='Baseline (Non-FL)', linewidth=2)\n",
        "ax1.plot(range(1, len(fl_history) + 1),\n",
        "         [x * 100 for x in fl_history],\n",
        "         'r-', label=f'FL ({N_CLIENTS} clients)', linewidth=2)\n",
        "ax1.set_xlabel('Epoch / Round')\n",
        "ax1.set_ylabel('Test Accuracy (%)')\n",
        "ax1.set_title('Accuracy: FL vs Non-FL')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Clients vs Performance\n",
        "clients = [r['n_clients'] for r in optimal_results]\n",
        "accs = [r['accuracy'] * 100 for r in optimal_results]\n",
        "times = [r['time'] for r in optimal_results]\n",
        "\n",
        "ax2_twin = ax2.twinx()\n",
        "ax2.bar([x - 0.2 for x in clients], accs, width=0.4,\n",
        "        color='skyblue', label='Accuracy')\n",
        "ax2_twin.plot(clients, times, 'ro-', linewidth=2,\n",
        "              markersize=8, label='Time')\n",
        "\n",
        "ax2.set_xlabel('Number of Clients')\n",
        "ax2.set_ylabel('Accuracy (%)', color='blue')\n",
        "ax2_twin.set_ylabel('Training Time (s)', color='red')\n",
        "ax2.set_title('Optimal Number of Clients')\n",
        "ax2.tick_params(axis='y', labelcolor='blue')\n",
        "ax2_twin.tick_params(axis='y', labelcolor='red')\n",
        "ax2.legend(loc='upper left')\n",
        "ax2_twin.legend(loc='upper right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('federated_qcnn_comparison.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"\\n✓ Visualization saved as 'federated_qcnn_comparison.png'\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GTt21fQjnFur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}